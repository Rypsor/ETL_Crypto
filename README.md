This repository contains a personal ETL (Extract, Transform, Load) project that I developed to demonstrate my skills in data engineering and cloud technologies.

The pipeline is designed to be scalable, cost-efficient, and production-ready, following industry practices:

Extraction: Collecting raw data from APIs or files and storing it in Amazon S3.

Transformation: Cleaning, enriching, and structuring the data with PySpark, optimized into Parquet format.

Loading: Moving the processed data into Amazon Redshift for analytics and reporting.

Orchestration: Using Apache Airflow (Docker) to schedule, automate, and monitor pipeline runs.

Infrastructure as Code: Provisioning resources with Terraform, ensuring reproducibility and cloud-native workflows.

The purpose of this project is to showcase my ability to build modern data pipelines that can evolve from local development to a cloud-based production setup. It reflects my learning journey in data engineering, DevOps practices, and cloud computing while keeping costs low through AWS Free Tier services.
